{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T14:10:12.684046Z",
     "start_time": "2025-02-09T14:10:12.197920Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8df197bfac13ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:19:57.079105Z",
     "start_time": "2024-12-23T20:19:57.070937Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795bc03809b0bd76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:19:57.201303Z",
     "start_time": "2024-12-23T20:19:57.192566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current location:  /Users/denys_herasymuk/Research/NYU/VirnyFlow_Project/Code/virny-flow-experiments\n"
     ]
    }
   ],
   "source": [
    "cur_folder_name = os.getcwd().split('/')[-1]\n",
    "if cur_folder_name != \"virny-flow-experiments\":\n",
    "    os.chdir(\"../../..\")\n",
    "\n",
    "print('Current location: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a3d9884591691",
   "metadata": {},
   "source": [
    "# Case Studies Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7355907a9d999068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:19:57.640626Z",
     "start_time": "2024-12-23T20:19:57.275517Z"
    }
   },
   "outputs": [],
   "source": [
    "from duckdb import query as sqldf\n",
    "from source.visualizations.use_case_queries import get_best_lps_per_exp_config\n",
    "from source.visualizations.cost_model_viz import create_box_plot_per_dataset_and_case_study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c1d9ce97655db",
   "metadata": {},
   "source": [
    "## Prepare data for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7010681237f5c94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:34.264169Z",
     "start_time": "2024-12-23T20:20:34.250698Z"
    }
   },
   "outputs": [],
   "source": [
    "SECRETS_PATH = os.path.join(os.getcwd(), \"scripts\", \"configs\", \"secrets.env\")\n",
    "EXP_NAME = 'case_studies_exp'\n",
    "DATASET_NAME = 'law_school'\n",
    "EXP_CONFIG_NAMES = [\n",
    "    \"case_studies_exp_law_school_cs1_w_acc_0_25_w_fair_0_75\",\n",
    "    \"case_studies_exp_law_school_cs1_w_acc_0_5_w_fair_0_5\",\n",
    "    \"case_studies_exp_law_school_cs1_w_acc_0_75_w_fair_0_25\",\n",
    "    \"case_studies_exp_law_school_cs2_w_acc_0_25_w_fair_0_75\",\n",
    "    \"case_studies_exp_law_school_cs2_w_acc_0_5_w_fair_0_5\",\n",
    "    \"case_studies_exp_law_school_cs2_w_acc_0_75_w_fair_0_25\",\n",
    "    \"case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_25_w_fair2_0_5\",\n",
    "    \"case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_5_w_fair2_0_25\",\n",
    "    \"case_studies_exp_law_school_cs3_w_acc_0_33_w_fair1_0_33_w_fair2_0_33\",\n",
    "    \"case_studies_exp_law_school_cs3_w_acc_0_5_w_fair1_0_25_w_fair2_0_25\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8219596d0fdfa897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:42.790084Z",
     "start_time": "2024-12-23T20:20:34.696383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metrics for case_studies_exp_law_school_cs1_w_acc_0_25_w_fair_0_75...\n",
      "best_pps_per_lp_and_run_num_df.shape: (72, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs1_w_acc_0_25_w_fair_0_75\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs1_w_acc_0_5_w_fair_0_5...\n",
      "best_pps_per_lp_and_run_num_df.shape: (63, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs1_w_acc_0_5_w_fair_0_5\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs1_w_acc_0_75_w_fair_0_25...\n",
      "best_pps_per_lp_and_run_num_df.shape: (99, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs1_w_acc_0_75_w_fair_0_25\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs2_w_acc_0_25_w_fair_0_75...\n",
      "best_pps_per_lp_and_run_num_df.shape: (90, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs2_w_acc_0_25_w_fair_0_75\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs2_w_acc_0_5_w_fair_0_5...\n",
      "best_pps_per_lp_and_run_num_df.shape: (81, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs2_w_acc_0_5_w_fair_0_5\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs2_w_acc_0_75_w_fair_0_25...\n",
      "best_pps_per_lp_and_run_num_df.shape: (90, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs2_w_acc_0_75_w_fair_0_25\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_25_w_fair2_0_5...\n",
      "best_pps_per_lp_and_run_num_df.shape: (90, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_25_w_fair2_0_5\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_5_w_fair2_0_25...\n",
      "best_pps_per_lp_and_run_num_df.shape: (72, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs3_w_acc_0_25_w_fair1_0_5_w_fair2_0_25\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs3_w_acc_0_33_w_fair1_0_33_w_fair2_0_33...\n",
      "best_pps_per_lp_and_run_num_df.shape: (81, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs3_w_acc_0_33_w_fair1_0_33_w_fair2_0_33\n",
      "\n",
      "Extracting metrics for case_studies_exp_law_school_cs3_w_acc_0_5_w_fair1_0_25_w_fair2_0_25...\n",
      "best_pps_per_lp_and_run_num_df.shape: (81, 19)\n",
      "best_lp_per_run_all.shape: (45, 19)\n",
      "Extracted metrics for case_studies_exp_law_school_cs3_w_acc_0_5_w_fair1_0_25_w_fair2_0_25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lp_metrics_per_exp_config_df = get_best_lps_per_exp_config(secrets_path=SECRETS_PATH, exp_config_names=EXP_CONFIG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71f069356c66137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:45.663268Z",
     "start_time": "2024-12-23T20:20:45.646563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lp_metrics_per_exp_config_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16f8184380a7aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:45.805124Z",
     "start_time": "2024-12-23T20:20:45.786908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subgroup</th>\n",
       "      <th>exp_config_name</th>\n",
       "      <th>run_num</th>\n",
       "      <th>logical_pipeline_uuid</th>\n",
       "      <th>physical_pipeline_uuid</th>\n",
       "      <th>compound_pp_quality</th>\n",
       "      <th>metric</th>\n",
       "      <th>model_name</th>\n",
       "      <th>runtime_in_mins</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>logical_pipeline_name</th>\n",
       "      <th>null_imputer_name</th>\n",
       "      <th>fairness_intervention_name</th>\n",
       "      <th>male&amp;race_dis</th>\n",
       "      <th>male&amp;race_priv</th>\n",
       "      <th>male_dis</th>\n",
       "      <th>male_priv</th>\n",
       "      <th>overall</th>\n",
       "      <th>race_dis</th>\n",
       "      <th>race_priv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>F1</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949473</td>\n",
       "      <td>0.928340</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>0.840775</td>\n",
       "      <td>0.956887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subgroup                                    exp_config_name  run_num  \\\n",
       "0         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "1         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "2         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "3         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "4         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "\n",
       "subgroup                              logical_pipeline_uuid  \\\n",
       "0         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "1         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "2         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "3         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "4         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "\n",
       "subgroup                physical_pipeline_uuid  compound_pp_quality    metric  \\\n",
       "0         8a0d4479-0156-4879-8293-2303178691de             0.984829  Accuracy   \n",
       "1         8a0d4479-0156-4879-8293-2303178691de             0.984829        F1   \n",
       "2         8a0d4479-0156-4879-8293-2303178691de             0.984829       FNR   \n",
       "3         8a0d4479-0156-4879-8293-2303178691de             0.984829       FPR   \n",
       "4         8a0d4479-0156-4879-8293-2303178691de             0.984829       PPV   \n",
       "\n",
       "subgroup   model_name  runtime_in_mins dataset_name  \\\n",
       "0         gandalf_clf         1.614088   law_school   \n",
       "1         gandalf_clf         1.614088   law_school   \n",
       "2         gandalf_clf         1.614088   law_school   \n",
       "3         gandalf_clf         1.614088   law_school   \n",
       "4         gandalf_clf         1.614088   law_school   \n",
       "\n",
       "subgroup                      logical_pipeline_name null_imputer_name  \\\n",
       "0         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "1         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "2         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "3         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "4         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "\n",
       "subgroup fairness_intervention_name  male&race_dis  male&race_priv  male_dis  \\\n",
       "0          NO_FAIRNESS_INTERVENTION       0.702128        0.903805  0.866263   \n",
       "1          NO_FAIRNESS_INTERVENTION       0.825000        0.949473  0.928340   \n",
       "2          NO_FAIRNESS_INTERVENTION       0.000000        0.000000  0.000000   \n",
       "3          NO_FAIRNESS_INTERVENTION       1.000000        1.000000  1.000000   \n",
       "4          NO_FAIRNESS_INTERVENTION       0.702128        0.903805  0.866263   \n",
       "\n",
       "subgroup  male_priv   overall  race_dis  race_priv  \n",
       "0          0.900555  0.885577  0.725291   0.917339  \n",
       "1          0.947676  0.939317  0.840775   0.956887  \n",
       "2          0.000000  0.000000  0.000000   0.000000  \n",
       "3          1.000000  1.000000  1.000000   1.000000  \n",
       "4          0.900555  0.885577  0.725291   0.917339  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lp_metrics_per_exp_config_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946fc971262e17f",
   "metadata": {},
   "source": [
    "## Get the best lp per exp config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb71d75c8c46e494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:46.019742Z",
     "start_time": "2024-12-23T20:20:46.003196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_config_name</th>\n",
       "      <th>logical_pipeline_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_5_w_fa...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_5_w_fa...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_33_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;rf_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      exp_config_name  \\\n",
       "0   case_studies_exp_law_school_cs2_w_acc_0_75_w_f...   \n",
       "1   case_studies_exp_law_school_cs1_w_acc_0_75_w_f...   \n",
       "2   case_studies_exp_law_school_cs2_w_acc_0_5_w_fa...   \n",
       "3   case_studies_exp_law_school_cs3_w_acc_0_25_w_f...   \n",
       "4   case_studies_exp_law_school_cs1_w_acc_0_25_w_f...   \n",
       "5   case_studies_exp_law_school_cs1_w_acc_0_5_w_fa...   \n",
       "6   case_studies_exp_law_school_cs2_w_acc_0_25_w_f...   \n",
       "7   case_studies_exp_law_school_cs3_w_acc_0_25_w_f...   \n",
       "8   case_studies_exp_law_school_cs3_w_acc_0_33_w_f...   \n",
       "9   case_studies_exp_law_school_cs1_w_acc_0_75_w_f...   \n",
       "10  case_studies_exp_law_school_cs1_w_acc_0_75_w_f...   \n",
       "11  case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...   \n",
       "\n",
       "                        logical_pipeline_name  \n",
       "0   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "1   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "2   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "3   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "4   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "5   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "6   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "7   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "8   None&NO_FAIRNESS_INTERVENTION&gandalf_clf  \n",
       "9        None&NO_FAIRNESS_INTERVENTION&rf_clf  \n",
       "10     None&NO_FAIRNESS_INTERVENTION&lgbm_clf  \n",
       "11  None&NO_FAIRNESS_INTERVENTION&gandalf_clf  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best lp per exp config\n",
    "sqldf(\"\"\"\n",
    "    SELECT DISTINCT exp_config_name, logical_pipeline_name\n",
    "    FROM best_lp_metrics_per_exp_config_df\n",
    "\"\"\").to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd73cdbb27edb5",
   "metadata": {},
   "source": [
    "## Case study 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1e9837a2a9609d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:46.306831Z",
     "start_time": "2024-12-23T20:20:46.294047Z"
    }
   },
   "outputs": [],
   "source": [
    "CASE_STUDY_NAME = 'cs1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70d30d29ecbe50f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:46.670321Z",
     "start_time": "2024-12-23T20:20:46.646870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subgroup</th>\n",
       "      <th>exp_config_name</th>\n",
       "      <th>run_num</th>\n",
       "      <th>logical_pipeline_uuid</th>\n",
       "      <th>physical_pipeline_uuid</th>\n",
       "      <th>compound_pp_quality</th>\n",
       "      <th>metric</th>\n",
       "      <th>model_name</th>\n",
       "      <th>runtime_in_mins</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>logical_pipeline_name</th>\n",
       "      <th>null_imputer_name</th>\n",
       "      <th>fairness_intervention_name</th>\n",
       "      <th>male&amp;race_dis</th>\n",
       "      <th>male&amp;race_priv</th>\n",
       "      <th>male_dis</th>\n",
       "      <th>male_priv</th>\n",
       "      <th>overall</th>\n",
       "      <th>race_dis</th>\n",
       "      <th>race_priv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>F1</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949473</td>\n",
       "      <td>0.928340</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>0.840775</td>\n",
       "      <td>0.956887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>8a0d4479-0156-4879-8293-2303178691de</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.614088</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...</td>\n",
       "      <td>8d3a2cca-99b1-4b05-b7a2-47b978cf3715</td>\n",
       "      <td>0.959878</td>\n",
       "      <td>PPV</td>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.923945</td>\n",
       "      <td>0.907017</td>\n",
       "      <td>0.921945</td>\n",
       "      <td>0.915394</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.932988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...</td>\n",
       "      <td>8d3a2cca-99b1-4b05-b7a2-47b978cf3715</td>\n",
       "      <td>0.959878</td>\n",
       "      <td>Sample_Size</td>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3819.000000</td>\n",
       "      <td>1835.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>4160.000000</td>\n",
       "      <td>637.000000</td>\n",
       "      <td>3523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...</td>\n",
       "      <td>8d3a2cca-99b1-4b05-b7a2-47b978cf3715</td>\n",
       "      <td>0.959878</td>\n",
       "      <td>Selection-Rate</td>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.803519</td>\n",
       "      <td>0.974339</td>\n",
       "      <td>0.955313</td>\n",
       "      <td>0.964301</td>\n",
       "      <td>0.960337</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.986943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...</td>\n",
       "      <td>8d3a2cca-99b1-4b05-b7a2-47b978cf3715</td>\n",
       "      <td>0.959878</td>\n",
       "      <td>TNR</td>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.471154</td>\n",
       "      <td>0.186782</td>\n",
       "      <td>0.265766</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.110687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>case_studies_exp_law_school_cs1_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...</td>\n",
       "      <td>8d3a2cca-99b1-4b05-b7a2-47b978cf3715</td>\n",
       "      <td>0.959878</td>\n",
       "      <td>TPR</td>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;lgbm_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.990493</td>\n",
       "      <td>0.985741</td>\n",
       "      <td>0.986635</td>\n",
       "      <td>0.986246</td>\n",
       "      <td>0.923937</td>\n",
       "      <td>0.994787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "subgroup                                    exp_config_name  run_num  \\\n",
       "0         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "1         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "2         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "3         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "4         case_studies_exp_law_school_cs1_w_acc_0_25_w_f...        1   \n",
       "..                                                      ...      ...   \n",
       "85        case_studies_exp_law_school_cs1_w_acc_0_75_w_f...        5   \n",
       "86        case_studies_exp_law_school_cs1_w_acc_0_75_w_f...        5   \n",
       "87        case_studies_exp_law_school_cs1_w_acc_0_75_w_f...        5   \n",
       "88        case_studies_exp_law_school_cs1_w_acc_0_75_w_f...        5   \n",
       "89        case_studies_exp_law_school_cs1_w_acc_0_75_w_f...        5   \n",
       "\n",
       "subgroup                              logical_pipeline_uuid  \\\n",
       "0         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "1         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "2         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "3         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "4         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "..                                                      ...   \n",
       "85        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...   \n",
       "86        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...   \n",
       "87        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...   \n",
       "88        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...   \n",
       "89        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV...   \n",
       "\n",
       "subgroup                physical_pipeline_uuid  compound_pp_quality  \\\n",
       "0         8a0d4479-0156-4879-8293-2303178691de             0.984829   \n",
       "1         8a0d4479-0156-4879-8293-2303178691de             0.984829   \n",
       "2         8a0d4479-0156-4879-8293-2303178691de             0.984829   \n",
       "3         8a0d4479-0156-4879-8293-2303178691de             0.984829   \n",
       "4         8a0d4479-0156-4879-8293-2303178691de             0.984829   \n",
       "..                                         ...                  ...   \n",
       "85        8d3a2cca-99b1-4b05-b7a2-47b978cf3715             0.959878   \n",
       "86        8d3a2cca-99b1-4b05-b7a2-47b978cf3715             0.959878   \n",
       "87        8d3a2cca-99b1-4b05-b7a2-47b978cf3715             0.959878   \n",
       "88        8d3a2cca-99b1-4b05-b7a2-47b978cf3715             0.959878   \n",
       "89        8d3a2cca-99b1-4b05-b7a2-47b978cf3715             0.959878   \n",
       "\n",
       "subgroup          metric   model_name  runtime_in_mins dataset_name  \\\n",
       "0               Accuracy  gandalf_clf         1.614088   law_school   \n",
       "1                     F1  gandalf_clf         1.614088   law_school   \n",
       "2                    FNR  gandalf_clf         1.614088   law_school   \n",
       "3                    FPR  gandalf_clf         1.614088   law_school   \n",
       "4                    PPV  gandalf_clf         1.614088   law_school   \n",
       "..                   ...          ...              ...          ...   \n",
       "85                   PPV     lgbm_clf         0.124785   law_school   \n",
       "86           Sample_Size     lgbm_clf         0.124785   law_school   \n",
       "87        Selection-Rate     lgbm_clf         0.124785   law_school   \n",
       "88                   TNR     lgbm_clf         0.124785   law_school   \n",
       "89                   TPR     lgbm_clf         0.124785   law_school   \n",
       "\n",
       "subgroup                      logical_pipeline_name null_imputer_name  \\\n",
       "0         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "1         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "2         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "3         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "4         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "..                                              ...               ...   \n",
       "85           None&NO_FAIRNESS_INTERVENTION&lgbm_clf              None   \n",
       "86           None&NO_FAIRNESS_INTERVENTION&lgbm_clf              None   \n",
       "87           None&NO_FAIRNESS_INTERVENTION&lgbm_clf              None   \n",
       "88           None&NO_FAIRNESS_INTERVENTION&lgbm_clf              None   \n",
       "89           None&NO_FAIRNESS_INTERVENTION&lgbm_clf              None   \n",
       "\n",
       "subgroup fairness_intervention_name  male&race_dis  male&race_priv  \\\n",
       "0          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "1          NO_FAIRNESS_INTERVENTION       0.825000        0.949473   \n",
       "2          NO_FAIRNESS_INTERVENTION       0.000000        0.000000   \n",
       "3          NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "4          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "..                              ...            ...             ...   \n",
       "85         NO_FAIRNESS_INTERVENTION       0.799270        0.923945   \n",
       "86         NO_FAIRNESS_INTERVENTION     341.000000     3819.000000   \n",
       "87         NO_FAIRNESS_INTERVENTION       0.803519        0.974339   \n",
       "88         NO_FAIRNESS_INTERVENTION       0.471154        0.186782   \n",
       "89         NO_FAIRNESS_INTERVENTION       0.924051        0.990493   \n",
       "\n",
       "subgroup     male_dis    male_priv      overall    race_dis    race_priv  \n",
       "0            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "1            0.928340     0.947676     0.939317    0.840775     0.956887  \n",
       "2            0.000000     0.000000     0.000000    0.000000     0.000000  \n",
       "3            1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "4            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "..                ...          ...          ...         ...          ...  \n",
       "85           0.907017     0.921945     0.915394    0.797297     0.932988  \n",
       "86        1835.000000  2325.000000  4160.000000  637.000000  3523.000000  \n",
       "87           0.955313     0.964301     0.960337    0.813187     0.986943  \n",
       "88           0.265766     0.239130     0.252212    0.447368     0.110687  \n",
       "89           0.985741     0.986635     0.986246    0.923937     0.994787  \n",
       "\n",
       "[135 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_plot = best_lp_metrics_per_exp_config_df\n",
    "to_plot[(to_plot['dataset_name'] == DATASET_NAME) & (to_plot['exp_config_name'].str.contains(CASE_STUDY_NAME, case=False, na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d13a28d01ca267b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:46.998203Z",
     "start_time": "2024-12-23T20:20:46.935776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-02172edb98ab4c13b4d29fb4b1725dc3\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-02172edb98ab4c13b4d29fb4b1725dc3\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-02172edb98ab4c13b4d29fb4b1725dc3\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-9fe1efb6f56eb13ffd3f818dc6b45f68\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"overall\", \"scale\": {\"zero\": false}, \"title\": \"F1\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9fe1efb6f56eb13ffd3f818dc6b45f68\": [{\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8a0d4479-0156-4879-8293-2303178691de\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6140880666666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ebc6a20d-f1c4-4d8c-b481-135934f5f7a0\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7645659833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"3047a13d-3489-4ef3-980a-aac93d23107e\", \"compound_pp_quality\": 0.9856848063433056, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.3791204000000001, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.839344262295082, \"male&race_priv\": 0.9518405756988652, \"male_dis\": 0.9359492210040392, \"male_priv\": 0.9487414187643021, \"overall\": 0.9430832057172026, \"race_dis\": 0.8461538461538461, \"race_priv\": 0.9592140518011313}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"df917591-0446-4876-a360-ea78565f9c3b\", \"compound_pp_quality\": 0.9870355542208051, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.0558619333333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8436911487758946, \"male&race_priv\": 0.9565217391304348, \"male_dis\": 0.941348117887365, \"male_priv\": 0.9547212741751991, \"overall\": 0.9488621835847609, \"race_dis\": 0.8565965583173997, \"race_priv\": 0.9631050767414404}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"434a94fe-8419-463f-8e2c-7d929b19ecc8\", \"compound_pp_quality\": 0.9859371383589082, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 4.2953388666666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.823943661971831, \"male&race_priv\": 0.9533892479032037, \"male_dis\": 0.9371362048894063, \"male_priv\": 0.9493757094211124, \"overall\": 0.9440122433363092, \"race_dis\": 0.8316086547507056, \"race_priv\": 0.9616406019474771}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"490913b0-2ccd-4df9-b443-57ff29ad52d8\", \"compound_pp_quality\": 0.9698264525176687, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7098821833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8269230769230769, \"male&race_priv\": 0.949874686716792, \"male_dis\": 0.9294605809128631, \"male_priv\": 0.9481046931407943, \"overall\": 0.9400461183704842, \"race_dis\": 0.8430182133564614, \"race_priv\": 0.956861566210732}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8988aae8-b2cc-48e0-8d10-fb275799ffa5\", \"compound_pp_quality\": 0.9703595717085537, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.7548107666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8421052631578947, \"male&race_priv\": 0.9491005438572027, \"male_dis\": 0.9313782991202346, \"male_priv\": 0.9480430304417486, \"overall\": 0.9407378840467926, \"race_dis\": 0.8515021459227468, \"race_priv\": 0.956456002419111}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a5e2856e-ba6a-449a-b22b-677c3e3b7963\", \"compound_pp_quality\": 0.9716093260519217, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7404335166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8452579034941764, \"male&race_priv\": 0.9517470881863561, \"male_dis\": 0.9369212962962963, \"male_priv\": 0.9488179940325913, \"overall\": 0.9435556124408038, \"race_dis\": 0.8480436760691538, \"race_priv\": 0.9591897527554364}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ab5cc229-0969-4197-9f71-1d7f0142256b\", \"compound_pp_quality\": 0.9747894040945848, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 3.3777922499999997, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.879837067209776, \"male&race_priv\": 0.9568115539508402, \"male_dis\": 0.9462429462429462, \"male_priv\": 0.956300578034682, \"overall\": 0.9518980759230369, \"race_dis\": 0.8895768833849329, \"race_priv\": 0.9608805592741335}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"32e76f66-e158-46ba-b807-09616fd417b1\", \"compound_pp_quality\": 0.972509596511113, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.88451065, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8327272727272728, \"male&race_priv\": 0.9538546559823156, \"male_dis\": 0.9387635511280399, \"male_priv\": 0.9504, \"overall\": 0.9453004622496148, \"race_dis\": 0.8421052631578947, \"race_priv\": 0.9609582963620231}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"c67846e7-56cf-434b-9bc2-9f3385419e40\", \"compound_pp_quality\": 0.9549362854440765, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6153623000000001, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8269230769230769, \"male&race_priv\": 0.949874686716792, \"male_dis\": 0.9294605809128631, \"male_priv\": 0.9481046931407943, \"overall\": 0.9400461183704842, \"race_dis\": 0.8430182133564614, \"race_priv\": 0.956861566210732}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"86b00ee4-55fe-4a60-b8f5-6ef352a22d88\", \"compound_pp_quality\": 0.9562978411381011, \"metric\": \"F1\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.015418066666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8678571428571429, \"male&race_priv\": 0.9513361462728551, \"male_dis\": 0.9370337212772307, \"male_priv\": 0.9516091687890715, \"overall\": 0.9452411994784876, \"race_dis\": 0.8856088560885609, \"race_priv\": 0.9550561797752809}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"083cc59a-da5d-488f-a274-439d406a1645\", \"compound_pp_quality\": 0.9576789102395045, \"metric\": \"F1\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.162417, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8571428571428571, \"male&race_priv\": 0.9526751860693723, \"male_dis\": 0.9418364334219073, \"male_priv\": 0.9489153254023793, \"overall\": 0.9457909825384415, \"race_dis\": 0.8642220019821606, \"race_priv\": 0.958139534883721}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"98f9c7f9-06b1-4617-bdfa-39a206546886\", \"compound_pp_quality\": 0.9635168696229272, \"metric\": \"F1\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.013534316666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8716904276985743, \"male&race_priv\": 0.9578597172165234, \"male_dis\": 0.9478054567022538, \"male_priv\": 0.9559196861297022, \"overall\": 0.9523685918234912, \"race_dis\": 0.8838643371017472, \"race_priv\": 0.9622697563874034}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV9jbGY=\", \"physical_pipeline_uuid\": \"8d3a2cca-99b1-4b05-b7a2-47b978cf3715\", \"compound_pp_quality\": 0.9598782151044497, \"metric\": \"F1\", \"model_name\": \"lgbm_clf\", \"runtime_in_mins\": 0.1247851, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&lgbm_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8571428571428571, \"male&race_priv\": 0.9560622914349277, \"male_dis\": 0.9447415329768271, \"male_priv\": 0.9531934516947198, \"overall\": 0.9495001947293262, \"race_dis\": 0.8559585492227979, \"race_priv\": 0.9628970020777678}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d9bb4d6ab4cf37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:47.162044Z",
     "start_time": "2024-12-23T20:20:47.111350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2c44bf12f5314ba4a0629cb1d1caeb89\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2c44bf12f5314ba4a0629cb1d1caeb89\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2c44bf12f5314ba4a0629cb1d1caeb89\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-d07b9afa01dc4f45bf82ca3064d6ba7c\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"disparity_metric_value\", \"scale\": {\"zero\": false}, \"title\": \"SRD\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-d07b9afa01dc4f45bf82ca3064d6ba7c\": [{\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8a0d4479-0156-4879-8293-2303178691de\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6140880666666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ebc6a20d-f1c4-4d8c-b481-135934f5f7a0\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7645659833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"3047a13d-3489-4ef3-980a-aac93d23107e\", \"compound_pp_quality\": 0.9856848063433056, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.3791204000000001, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9721448467966574, \"male&race_priv\": 0.9960536700868192, \"male_dis\": 0.994054054054054, \"male_priv\": 0.9939393939393939, \"overall\": 0.9939903846153846, \"race_dis\": 0.9667170953101362, \"race_priv\": 0.9991426121749071, \"disparity_metric_value\": 0.0001146601146601034}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"df917591-0446-4876-a360-ea78565f9c3b\", \"compound_pp_quality\": 0.9870355542208051, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.0558619333333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9148936170212766, \"male&race_priv\": 0.990602975724354, \"male_dis\": 0.9847494553376906, \"male_priv\": 0.9845094664371773, \"overall\": 0.9846153846153847, \"race_dis\": 0.9141965678627145, \"race_priv\": 0.9974424552429667, \"disparity_metric_value\": 0.00023998890051335486}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"434a94fe-8419-463f-8e2c-7d929b19ecc8\", \"compound_pp_quality\": 0.9859371383589082, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 4.2953388666666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9706744868035191, \"male&race_priv\": 0.9955485729248494, \"male_dis\": 0.9934604904632153, \"male_priv\": 0.9935483870967742, \"overall\": 0.9935096153846154, \"race_dis\": 0.967032967032967, \"race_priv\": 0.9982969060459835, \"disparity_metric_value\": -8.789663355890376e-05}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"490913b0-2ccd-4df9-b443-57ff29ad52d8\", \"compound_pp_quality\": 0.9698264525176687, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7098821833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9574468085106383, \"male&race_priv\": 0.9941860465116279, \"male_dis\": 0.9906439185470556, \"male_priv\": 0.9910371318822023, \"overall\": 0.9908653846153846, \"race_dis\": 0.9505813953488372, \"race_priv\": 0.9988479262672811, \"disparity_metric_value\": -0.00039321333514674706}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8988aae8-b2cc-48e0-8d10-fb275799ffa5\", \"compound_pp_quality\": 0.9703595717085537, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.7548107666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9400544959128065, \"male&race_priv\": 0.9897179013973109, \"male_dis\": 0.9853260869565217, \"male_priv\": 0.9853448275862069, \"overall\": 0.9853365384615385, \"race_dis\": 0.9314285714285714, \"race_priv\": 0.996242774566474, \"disparity_metric_value\": -1.8740629685209953e-05}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a5e2856e-ba6a-449a-b22b-677c3e3b7963\", \"compound_pp_quality\": 0.9716093260519217, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7404335166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.947075208913649, \"male&race_priv\": 0.9923704288345172, \"male_dis\": 0.9886486486486487, \"male_priv\": 0.9883116883116884, \"overall\": 0.9884615384615385, \"race_dis\": 0.9379727685325264, \"race_priv\": 0.9979994284081166, \"disparity_metric_value\": 0.00033696033696029026}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ab5cc229-0969-4197-9f71-1d7f0142256b\", \"compound_pp_quality\": 0.9747894040945848, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 3.3777922499999997, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.7933130699088146, \"male&race_priv\": 0.9671104150352389, \"male_dis\": 0.9520697167755992, \"male_priv\": 0.9543889845094664, \"overall\": 0.9533653846153847, \"race_dis\": 0.7940717628705148, \"race_priv\": 0.9823813583404376, \"disparity_metric_value\": -0.0023192677338672585}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"32e76f66-e158-46ba-b807-09616fd417b1\", \"compound_pp_quality\": 0.972509596511113, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.88451065, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9178885630498533, \"male&race_priv\": 0.9863838701230688, \"male_dis\": 0.9809264305177112, \"male_priv\": 0.9806451612903225, \"overall\": 0.9807692307692307, \"race_dis\": 0.9089481946624803, \"race_priv\": 0.9937553221686063, \"disparity_metric_value\": 0.00028126922738869187}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"c67846e7-56cf-434b-9bc2-9f3385419e40\", \"compound_pp_quality\": 0.9549362854440765, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6153623000000001, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.9574468085106383, \"male&race_priv\": 0.9941860465116279, \"male_dis\": 0.9906439185470556, \"male_priv\": 0.9910371318822023, \"overall\": 0.9908653846153846, \"race_dis\": 0.9505813953488372, \"race_priv\": 0.9988479262672811, \"disparity_metric_value\": -0.00039321333514674706}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"86b00ee4-55fe-4a60-b8f5-6ef352a22d88\", \"compound_pp_quality\": 0.9562978411381011, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.015418066666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8092643051771117, \"male&race_priv\": 0.9736356446084893, \"male_dis\": 0.9532608695652174, \"male_priv\": 0.9637931034482758, \"overall\": 0.9591346153846154, \"race_dis\": 0.8157142857142857, \"race_priv\": 0.988150289017341, \"disparity_metric_value\": -0.010532233883058462}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"083cc59a-da5d-488f-a274-439d406a1645\", \"compound_pp_quality\": 0.9576789102395045, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.162417, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8133704735376045, \"male&race_priv\": 0.9684293606945541, \"male_dis\": 0.9513513513513514, \"male_priv\": 0.958008658008658, \"overall\": 0.9550480769230769, \"race_dis\": 0.8018154311649016, \"race_priv\": 0.9839954272649328, \"disparity_metric_value\": -0.006657306657306572}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mcmZfY2xm\", \"physical_pipeline_uuid\": \"98f9c7f9-06b1-4617-bdfa-39a206546886\", \"compound_pp_quality\": 0.9635168696229272, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"rf_clf\", \"runtime_in_mins\": 0.013534316666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&rf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.7933130699088146, \"male&race_priv\": 0.9705037849125555, \"male_dis\": 0.9547930283224401, \"male_priv\": 0.9578313253012049, \"overall\": 0.9564903846153846, \"race_dis\": 0.8003120124804992, \"race_priv\": 0.9849389030974709, \"disparity_metric_value\": -0.0030382969787647873}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mbGdibV9jbGY=\", \"physical_pipeline_uuid\": \"8d3a2cca-99b1-4b05-b7a2-47b978cf3715\", \"compound_pp_quality\": 0.9598782151044497, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"lgbm_clf\", \"runtime_in_mins\": 0.1247851, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&lgbm_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8035190615835777, \"male&race_priv\": 0.9743388321550144, \"male_dis\": 0.9553133514986376, \"male_priv\": 0.9643010752688173, \"overall\": 0.9603365384615384, \"race_dis\": 0.8131868131868132, \"race_priv\": 0.9869429463525404, \"disparity_metric_value\": -0.008987723770179645}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"Selection_Rate_Difference\",\n",
    "                                           group=\"male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115acb81b4db554c",
   "metadata": {},
   "source": [
    "## Case study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddbf2df1ce330560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:47.316635Z",
     "start_time": "2024-12-23T20:20:47.303821Z"
    }
   },
   "outputs": [],
   "source": [
    "CASE_STUDY_NAME = 'cs2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd89188274598a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:47.461991Z",
     "start_time": "2024-12-23T20:20:47.435798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subgroup</th>\n",
       "      <th>exp_config_name</th>\n",
       "      <th>run_num</th>\n",
       "      <th>logical_pipeline_uuid</th>\n",
       "      <th>physical_pipeline_uuid</th>\n",
       "      <th>compound_pp_quality</th>\n",
       "      <th>metric</th>\n",
       "      <th>model_name</th>\n",
       "      <th>runtime_in_mins</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>logical_pipeline_name</th>\n",
       "      <th>null_imputer_name</th>\n",
       "      <th>fairness_intervention_name</th>\n",
       "      <th>male&amp;race_dis</th>\n",
       "      <th>male&amp;race_priv</th>\n",
       "      <th>male_dis</th>\n",
       "      <th>male_priv</th>\n",
       "      <th>overall</th>\n",
       "      <th>race_dis</th>\n",
       "      <th>race_priv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>aee3bce1-aee2-4c04-a84d-84ad31bf0b34</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.576960</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>aee3bce1-aee2-4c04-a84d-84ad31bf0b34</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>F1</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.576960</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949473</td>\n",
       "      <td>0.928340</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>0.840775</td>\n",
       "      <td>0.956887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>aee3bce1-aee2-4c04-a84d-84ad31bf0b34</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.576960</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>aee3bce1-aee2-4c04-a84d-84ad31bf0b34</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.576960</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>aee3bce1-aee2-4c04-a84d-84ad31bf0b34</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.576960</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d</td>\n",
       "      <td>0.956914</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>2.897356</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.695015</td>\n",
       "      <td>0.908877</td>\n",
       "      <td>0.879019</td>\n",
       "      <td>0.901075</td>\n",
       "      <td>0.891346</td>\n",
       "      <td>0.701727</td>\n",
       "      <td>0.925632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d</td>\n",
       "      <td>0.956914</td>\n",
       "      <td>Sample_Size</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>2.897356</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3819.000000</td>\n",
       "      <td>1835.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>4160.000000</td>\n",
       "      <td>637.000000</td>\n",
       "      <td>3523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d</td>\n",
       "      <td>0.956914</td>\n",
       "      <td>Selection-Rate</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>2.897356</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d</td>\n",
       "      <td>0.956914</td>\n",
       "      <td>TNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>2.897356</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>case_studies_exp_law_school_cs2_w_acc_0_75_w_f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d</td>\n",
       "      <td>0.956914</td>\n",
       "      <td>TPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>2.897356</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "subgroup                                    exp_config_name  run_num  \\\n",
       "0         case_studies_exp_law_school_cs2_w_acc_0_25_w_f...        1   \n",
       "1         case_studies_exp_law_school_cs2_w_acc_0_25_w_f...        1   \n",
       "2         case_studies_exp_law_school_cs2_w_acc_0_25_w_f...        1   \n",
       "3         case_studies_exp_law_school_cs2_w_acc_0_25_w_f...        1   \n",
       "4         case_studies_exp_law_school_cs2_w_acc_0_25_w_f...        1   \n",
       "..                                                      ...      ...   \n",
       "76        case_studies_exp_law_school_cs2_w_acc_0_75_w_f...        5   \n",
       "77        case_studies_exp_law_school_cs2_w_acc_0_75_w_f...        5   \n",
       "78        case_studies_exp_law_school_cs2_w_acc_0_75_w_f...        5   \n",
       "79        case_studies_exp_law_school_cs2_w_acc_0_75_w_f...        5   \n",
       "80        case_studies_exp_law_school_cs2_w_acc_0_75_w_f...        5   \n",
       "\n",
       "subgroup                              logical_pipeline_uuid  \\\n",
       "0         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "1         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "2         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "3         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "4         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "..                                                      ...   \n",
       "76        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "77        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "78        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "79        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "80        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "\n",
       "subgroup                physical_pipeline_uuid  compound_pp_quality  \\\n",
       "0         aee3bce1-aee2-4c04-a84d-84ad31bf0b34             0.984829   \n",
       "1         aee3bce1-aee2-4c04-a84d-84ad31bf0b34             0.984829   \n",
       "2         aee3bce1-aee2-4c04-a84d-84ad31bf0b34             0.984829   \n",
       "3         aee3bce1-aee2-4c04-a84d-84ad31bf0b34             0.984829   \n",
       "4         aee3bce1-aee2-4c04-a84d-84ad31bf0b34             0.984829   \n",
       "..                                         ...                  ...   \n",
       "76        b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d             0.956914   \n",
       "77        b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d             0.956914   \n",
       "78        b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d             0.956914   \n",
       "79        b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d             0.956914   \n",
       "80        b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d             0.956914   \n",
       "\n",
       "subgroup          metric   model_name  runtime_in_mins dataset_name  \\\n",
       "0               Accuracy  gandalf_clf         1.576960   law_school   \n",
       "1                     F1  gandalf_clf         1.576960   law_school   \n",
       "2                    FNR  gandalf_clf         1.576960   law_school   \n",
       "3                    FPR  gandalf_clf         1.576960   law_school   \n",
       "4                    PPV  gandalf_clf         1.576960   law_school   \n",
       "..                   ...          ...              ...          ...   \n",
       "76                   PPV  gandalf_clf         2.897356   law_school   \n",
       "77           Sample_Size  gandalf_clf         2.897356   law_school   \n",
       "78        Selection-Rate  gandalf_clf         2.897356   law_school   \n",
       "79                   TNR  gandalf_clf         2.897356   law_school   \n",
       "80                   TPR  gandalf_clf         2.897356   law_school   \n",
       "\n",
       "subgroup                      logical_pipeline_name null_imputer_name  \\\n",
       "0         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "1         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "2         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "3         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "4         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "..                                              ...               ...   \n",
       "76        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "77        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "78        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "79        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "80        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "\n",
       "subgroup fairness_intervention_name  male&race_dis  male&race_priv  \\\n",
       "0          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "1          NO_FAIRNESS_INTERVENTION       0.825000        0.949473   \n",
       "2          NO_FAIRNESS_INTERVENTION       0.000000        0.000000   \n",
       "3          NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "4          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "..                              ...            ...             ...   \n",
       "76         NO_FAIRNESS_INTERVENTION       0.695015        0.908877   \n",
       "77         NO_FAIRNESS_INTERVENTION     341.000000     3819.000000   \n",
       "78         NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "79         NO_FAIRNESS_INTERVENTION       0.000000        0.000000   \n",
       "80         NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "\n",
       "subgroup     male_dis    male_priv      overall    race_dis    race_priv  \n",
       "0            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "1            0.928340     0.947676     0.939317    0.840775     0.956887  \n",
       "2            0.000000     0.000000     0.000000    0.000000     0.000000  \n",
       "3            1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "4            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "..                ...          ...          ...         ...          ...  \n",
       "76           0.879019     0.901075     0.891346    0.701727     0.925632  \n",
       "77        1835.000000  2325.000000  4160.000000  637.000000  3523.000000  \n",
       "78           1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "79           0.000000     0.000000     0.000000    0.000000     0.000000  \n",
       "80           1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "\n",
       "[135 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_plot = best_lp_metrics_per_exp_config_df\n",
    "to_plot[(to_plot['dataset_name'] == DATASET_NAME) & (to_plot['exp_config_name'].str.contains(CASE_STUDY_NAME, case=False, na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f0d8eb9dac608f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:47.988067Z",
     "start_time": "2024-12-23T20:20:47.946124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-43f239ad713540acb0576ca956ecef34\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-43f239ad713540acb0576ca956ecef34\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-43f239ad713540acb0576ca956ecef34\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-64ac8a83c7a2a230ef4d489b50bc1536\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"overall\", \"scale\": {\"zero\": false}, \"title\": \"F1\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-64ac8a83c7a2a230ef4d489b50bc1536\": [{\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"aee3bce1-aee2-4c04-a84d-84ad31bf0b34\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5769599833333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"76be8e9e-6ced-469b-a0ab-b892f92cdba3\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7804188666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"cf55d925-d743-46d4-8f99-e6693a5716bb\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.7831469, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"b37e011e-e6e6-4fcc-81df-fbfc06fcf3d3\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.3881492833333335, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"767029af-4e46-45cb-a385-a58b1b96ef9b\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6869695, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"486024af-d55d-4df2-8022-f37bc8c6972b\", \"compound_pp_quality\": 0.9696583375828659, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5809743166666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"18c69847-8449-4056-9b77-1d59270a62bd\", \"compound_pp_quality\": 0.9693877551020409, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6557023000000002, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"203c9e84-b2dc-4679-8623-0e97c5001b05\", \"compound_pp_quality\": 0.970805241063478, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.8036898666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"4ad062bc-93db-473e-90fe-aa58dfd6ddcd\", \"compound_pp_quality\": 0.9724828810550342, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6522827666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"9ab8bc91-4a99-495c-80bb-d37091e852fa\", \"compound_pp_quality\": 0.9712760549059481, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.4145593333333335, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"e45220cd-1048-4381-9272-ce28f656eb15\", \"compound_pp_quality\": 0.9544875063742988, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5876755333333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"385565bb-bf81-49ce-a451-b30b7384b01a\", \"compound_pp_quality\": 0.9540816326530612, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.1666843, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ec1f8ca4-8af7-4928-b021-14c79c7c5557\", \"compound_pp_quality\": 0.9562078615952169, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.7101306500000004, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8e5c5680-cd98-45a7-9f2f-72aabf6584fc\", \"compound_pp_quality\": 0.9587243215825514, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5361561833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d\", \"compound_pp_quality\": 0.9569140823589222, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.8973557666666663, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce615519a8374f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:48.273794Z",
     "start_time": "2024-12-23T20:20:48.219686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-907bbf86947d4cc8b21cdd37a941af09\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-907bbf86947d4cc8b21cdd37a941af09\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-907bbf86947d4cc8b21cdd37a941af09\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-49c5fe7391e8cd1f70051400ede22648\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"disparity_metric_value\", \"scale\": {\"zero\": false}, \"title\": \"SRD\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-49c5fe7391e8cd1f70051400ede22648\": [{\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"aee3bce1-aee2-4c04-a84d-84ad31bf0b34\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5769599833333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"76be8e9e-6ced-469b-a0ab-b892f92cdba3\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7804188666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"cf55d925-d743-46d4-8f99-e6693a5716bb\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.7831469, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"b37e011e-e6e6-4fcc-81df-fbfc06fcf3d3\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.3881492833333335, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair_0_75\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"767029af-4e46-45cb-a385-a58b1b96ef9b\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6869695, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"486024af-d55d-4df2-8022-f37bc8c6972b\", \"compound_pp_quality\": 0.9696583375828659, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5809743166666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"18c69847-8449-4056-9b77-1d59270a62bd\", \"compound_pp_quality\": 0.9693877551020409, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6557023000000002, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"203c9e84-b2dc-4679-8623-0e97c5001b05\", \"compound_pp_quality\": 0.970805241063478, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.8036898666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"4ad062bc-93db-473e-90fe-aa58dfd6ddcd\", \"compound_pp_quality\": 0.9724828810550342, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6522827666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"9ab8bc91-4a99-495c-80bb-d37091e852fa\", \"compound_pp_quality\": 0.9712760549059481, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.4145593333333335, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"e45220cd-1048-4381-9272-ce28f656eb15\", \"compound_pp_quality\": 0.9544875063742988, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5876755333333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"385565bb-bf81-49ce-a451-b30b7384b01a\", \"compound_pp_quality\": 0.9540816326530612, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.1666843, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ec1f8ca4-8af7-4928-b021-14c79c7c5557\", \"compound_pp_quality\": 0.9562078615952169, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.7101306500000004, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"8e5c5680-cd98-45a7-9f2f-72aabf6584fc\", \"compound_pp_quality\": 0.9587243215825514, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5361561833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_75_fair_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"b5c3ceac-9ffd-4dcf-8c82-ddbd76f67c2d\", \"compound_pp_quality\": 0.9569140823589222, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.8973557666666663, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"Selection_Rate_Difference\",\n",
    "                                           group=\"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e67f2eb175b94b",
   "metadata": {},
   "source": [
    "## Case study 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e2a26df3135c1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:49.432003Z",
     "start_time": "2024-12-23T20:20:49.412718Z"
    }
   },
   "outputs": [],
   "source": [
    "CASE_STUDY_NAME = 'cs3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6244bc043564593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:49.747960Z",
     "start_time": "2024-12-23T20:20:49.725331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subgroup</th>\n",
       "      <th>exp_config_name</th>\n",
       "      <th>run_num</th>\n",
       "      <th>logical_pipeline_uuid</th>\n",
       "      <th>physical_pipeline_uuid</th>\n",
       "      <th>compound_pp_quality</th>\n",
       "      <th>metric</th>\n",
       "      <th>model_name</th>\n",
       "      <th>runtime_in_mins</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>logical_pipeline_name</th>\n",
       "      <th>null_imputer_name</th>\n",
       "      <th>fairness_intervention_name</th>\n",
       "      <th>male&amp;race_dis</th>\n",
       "      <th>male&amp;race_priv</th>\n",
       "      <th>male_dis</th>\n",
       "      <th>male_priv</th>\n",
       "      <th>overall</th>\n",
       "      <th>race_dis</th>\n",
       "      <th>race_priv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>a9366a20-5bb4-4743-b105-7963ecf20c46</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.618944</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>a9366a20-5bb4-4743-b105-7963ecf20c46</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>F1</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.618944</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949473</td>\n",
       "      <td>0.928340</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>0.840775</td>\n",
       "      <td>0.956887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>a9366a20-5bb4-4743-b105-7963ecf20c46</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.618944</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>a9366a20-5bb4-4743-b105-7963ecf20c46</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>FPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.618944</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_25_w_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>a9366a20-5bb4-4743-b105-7963ecf20c46</td>\n",
       "      <td>0.984829</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.618944</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.903805</td>\n",
       "      <td>0.866263</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.725291</td>\n",
       "      <td>0.917339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e</td>\n",
       "      <td>0.971276</td>\n",
       "      <td>PPV</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.455248</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.695015</td>\n",
       "      <td>0.908877</td>\n",
       "      <td>0.879019</td>\n",
       "      <td>0.901075</td>\n",
       "      <td>0.891346</td>\n",
       "      <td>0.701727</td>\n",
       "      <td>0.925632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e</td>\n",
       "      <td>0.971276</td>\n",
       "      <td>Sample_Size</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.455248</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3819.000000</td>\n",
       "      <td>1835.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>4160.000000</td>\n",
       "      <td>637.000000</td>\n",
       "      <td>3523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e</td>\n",
       "      <td>0.971276</td>\n",
       "      <td>Selection-Rate</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.455248</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e</td>\n",
       "      <td>0.971276</td>\n",
       "      <td>TNR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.455248</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...</td>\n",
       "      <td>6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e</td>\n",
       "      <td>0.971276</td>\n",
       "      <td>TPR</td>\n",
       "      <td>gandalf_clf</td>\n",
       "      <td>1.455248</td>\n",
       "      <td>law_school</td>\n",
       "      <td>None&amp;NO_FAIRNESS_INTERVENTION&amp;gandalf_clf</td>\n",
       "      <td>None</td>\n",
       "      <td>NO_FAIRNESS_INTERVENTION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "subgroup                                    exp_config_name  run_num  \\\n",
       "0         case_studies_exp_law_school_cs3_w_acc_0_25_w_f...        1   \n",
       "1         case_studies_exp_law_school_cs3_w_acc_0_25_w_f...        1   \n",
       "2         case_studies_exp_law_school_cs3_w_acc_0_25_w_f...        1   \n",
       "3         case_studies_exp_law_school_cs3_w_acc_0_25_w_f...        1   \n",
       "4         case_studies_exp_law_school_cs3_w_acc_0_25_w_f...        1   \n",
       "..                                                      ...      ...   \n",
       "67        case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...        5   \n",
       "68        case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...        5   \n",
       "69        case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...        5   \n",
       "70        case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...        5   \n",
       "71        case_studies_exp_law_school_cs3_w_acc_0_5_w_fa...        5   \n",
       "\n",
       "subgroup                              logical_pipeline_uuid  \\\n",
       "0         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "1         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "2         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "3         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "4         Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "..                                                      ...   \n",
       "67        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "68        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "69        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "70        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "71        Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZG...   \n",
       "\n",
       "subgroup                physical_pipeline_uuid  compound_pp_quality  \\\n",
       "0         a9366a20-5bb4-4743-b105-7963ecf20c46             0.984829   \n",
       "1         a9366a20-5bb4-4743-b105-7963ecf20c46             0.984829   \n",
       "2         a9366a20-5bb4-4743-b105-7963ecf20c46             0.984829   \n",
       "3         a9366a20-5bb4-4743-b105-7963ecf20c46             0.984829   \n",
       "4         a9366a20-5bb4-4743-b105-7963ecf20c46             0.984829   \n",
       "..                                         ...                  ...   \n",
       "67        6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e             0.971276   \n",
       "68        6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e             0.971276   \n",
       "69        6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e             0.971276   \n",
       "70        6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e             0.971276   \n",
       "71        6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e             0.971276   \n",
       "\n",
       "subgroup          metric   model_name  runtime_in_mins dataset_name  \\\n",
       "0               Accuracy  gandalf_clf         1.618944   law_school   \n",
       "1                     F1  gandalf_clf         1.618944   law_school   \n",
       "2                    FNR  gandalf_clf         1.618944   law_school   \n",
       "3                    FPR  gandalf_clf         1.618944   law_school   \n",
       "4                    PPV  gandalf_clf         1.618944   law_school   \n",
       "..                   ...          ...              ...          ...   \n",
       "67                   PPV  gandalf_clf         1.455248   law_school   \n",
       "68           Sample_Size  gandalf_clf         1.455248   law_school   \n",
       "69        Selection-Rate  gandalf_clf         1.455248   law_school   \n",
       "70                   TNR  gandalf_clf         1.455248   law_school   \n",
       "71                   TPR  gandalf_clf         1.455248   law_school   \n",
       "\n",
       "subgroup                      logical_pipeline_name null_imputer_name  \\\n",
       "0         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "1         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "2         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "3         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "4         None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "..                                              ...               ...   \n",
       "67        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "68        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "69        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "70        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "71        None&NO_FAIRNESS_INTERVENTION&gandalf_clf              None   \n",
       "\n",
       "subgroup fairness_intervention_name  male&race_dis  male&race_priv  \\\n",
       "0          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "1          NO_FAIRNESS_INTERVENTION       0.825000        0.949473   \n",
       "2          NO_FAIRNESS_INTERVENTION       0.000000        0.000000   \n",
       "3          NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "4          NO_FAIRNESS_INTERVENTION       0.702128        0.903805   \n",
       "..                              ...            ...             ...   \n",
       "67         NO_FAIRNESS_INTERVENTION       0.695015        0.908877   \n",
       "68         NO_FAIRNESS_INTERVENTION     341.000000     3819.000000   \n",
       "69         NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "70         NO_FAIRNESS_INTERVENTION       0.000000        0.000000   \n",
       "71         NO_FAIRNESS_INTERVENTION       1.000000        1.000000   \n",
       "\n",
       "subgroup     male_dis    male_priv      overall    race_dis    race_priv  \n",
       "0            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "1            0.928340     0.947676     0.939317    0.840775     0.956887  \n",
       "2            0.000000     0.000000     0.000000    0.000000     0.000000  \n",
       "3            1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "4            0.866263     0.900555     0.885577    0.725291     0.917339  \n",
       "..                ...          ...          ...         ...          ...  \n",
       "67           0.879019     0.901075     0.891346    0.701727     0.925632  \n",
       "68        1835.000000  2325.000000  4160.000000  637.000000  3523.000000  \n",
       "69           1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "70           0.000000     0.000000     0.000000    0.000000     0.000000  \n",
       "71           1.000000     1.000000     1.000000    1.000000     1.000000  \n",
       "\n",
       "[180 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_plot = best_lp_metrics_per_exp_config_df\n",
    "to_plot[(to_plot['dataset_name'] == DATASET_NAME) & (to_plot['exp_config_name'].str.contains(CASE_STUDY_NAME, case=False, na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab71a6eec79cb801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:49.982860Z",
     "start_time": "2024-12-23T20:20:49.936707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-34024aca3d83450a944905df19991208\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-34024aca3d83450a944905df19991208\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-34024aca3d83450a944905df19991208\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-52c8627b99da544a338b414ec1e2bee1\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"overall\", \"scale\": {\"zero\": false}, \"title\": \"F1\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-52c8627b99da544a338b414ec1e2bee1\": [{\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a9366a20-5bb4-4743-b105-7963ecf20c46\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6189442666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"2326be7a-8b8a-4dc8-971f-d5909fe4476b\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4694882166666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6c206ee9-1583-437f-b436-5296f847f025\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.9273404833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f9a6b87a-bdf5-4a8c-8f65-0f31d56c6bc6\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.66953015, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f53a74fc-cb9d-4153-b9af-611df3acd7bf\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.649014166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"9ecb20b1-3d80-4c3a-9051-80bc4d6da81c\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5480944, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"765e8ea9-7b45-4b00-8e13-ac302a999fc8\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.9093007, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"c0bc36ff-a974-4193-a770-431197c698be\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.7651768666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0b5be382-b07c-4265-b6e5-97aaa2fb5f7f\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.0431344, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7eec8589-e4f6-42db-9649-3448f6e0353f\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.3439765166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"75ef957d-32a2-4110-8b5c-354831393ebe\", \"compound_pp_quality\": 0.9699745028046916, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4616962666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7aa403fe-2a72-487d-9cb5-58ae6c5a4750\", \"compound_pp_quality\": 0.969795918367347, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7881325833333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"5a02b83a-7822-40fb-a34a-2f3f552247b5\", \"compound_pp_quality\": 0.9707314591018954, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.8799559166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0f05c645-aec1-44e0-8c53-354e85a38632\", \"compound_pp_quality\": 0.9718387014963226, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5622455, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a94c59a1-d2c5-4486-9e86-ad14dcbd3d5f\", \"compound_pp_quality\": 0.9710421962379259, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.3591971833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"745ef5aa-d730-40d9-b6f6-1d463b8154eb\", \"compound_pp_quality\": 0.9696583375828659, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5803049, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.825, \"male&race_priv\": 0.9494725152692949, \"male_dis\": 0.9283397227956355, \"male_priv\": 0.9476757242308556, \"overall\": 0.9393166751657318, \"race_dis\": 0.8407750631844987, \"race_priv\": 0.9568874868559412}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ea9b7326-76d0-4013-a553-e57c2d4a3a0a\", \"compound_pp_quality\": 0.9693877551020409, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.9064114166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.834920634920635, \"male&race_priv\": 0.9478502080443828, \"male_dis\": 0.9292988070992144, \"male_priv\": 0.9461730638201227, \"overall\": 0.9387755102040817, \"race_dis\": 0.8458367683429514, \"race_priv\": 0.9557869322468688}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"4e41033f-a5d5-43f5-b448-edf568bc69a7\", \"compound_pp_quality\": 0.970805241063478, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.82476975, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8419354838709677, \"male&race_priv\": 0.9501450075956359, \"male_dis\": 0.9358642507909117, \"male_priv\": 0.9461678832116789, \"overall\": 0.9416104821269559, \"race_dis\": 0.8403508771929824, \"race_priv\": 0.9587858949561078}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"d55423d6-8f42-465f-ae93-e5c460fce626\", \"compound_pp_quality\": 0.9724828810550342, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.1378944166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8228980322003577, \"male&race_priv\": 0.9542786952367954, \"male_dis\": 0.9371924746743849, \"male_priv\": 0.9510268562401264, \"overall\": 0.9449657621100684, \"race_dis\": 0.8356039963669392, \"race_priv\": 0.9627118644067797}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e\", \"compound_pp_quality\": 0.9712760549059481, \"metric\": \"F1\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4552481833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 0.8200692041522492, \"male&race_priv\": 0.9522633744855967, \"male_dis\": 0.935614849187935, \"male_priv\": 0.9479638009049773, \"overall\": 0.9425521098118963, \"race_dis\": 0.8247232472324724, \"race_priv\": 0.9613797169811321}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3197106960d20d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:50.541275Z",
     "start_time": "2024-12-23T20:20:50.489068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-cb26b656110d40f7af2b4e612f8c9324\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cb26b656110d40f7af2b4e612f8c9324\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cb26b656110d40f7af2b4e612f8c9324\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-74932b799c91a26f5f1cca616279431d\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"disparity_metric_value\", \"scale\": {\"zero\": false}, \"title\": \"SRD\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-74932b799c91a26f5f1cca616279431d\": [{\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a9366a20-5bb4-4743-b105-7963ecf20c46\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6189442666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"2326be7a-8b8a-4dc8-971f-d5909fe4476b\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4694882166666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6c206ee9-1583-437f-b436-5296f847f025\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.9273404833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f9a6b87a-bdf5-4a8c-8f65-0f31d56c6bc6\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.66953015, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f53a74fc-cb9d-4153-b9af-611df3acd7bf\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.649014166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"9ecb20b1-3d80-4c3a-9051-80bc4d6da81c\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5480944, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"765e8ea9-7b45-4b00-8e13-ac302a999fc8\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.9093007, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"c0bc36ff-a974-4193-a770-431197c698be\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.7651768666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0b5be382-b07c-4265-b6e5-97aaa2fb5f7f\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.0431344, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7eec8589-e4f6-42db-9649-3448f6e0353f\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.3439765166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"75ef957d-32a2-4110-8b5c-354831393ebe\", \"compound_pp_quality\": 0.9699745028046916, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4616962666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7aa403fe-2a72-487d-9cb5-58ae6c5a4750\", \"compound_pp_quality\": 0.969795918367347, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7881325833333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"5a02b83a-7822-40fb-a34a-2f3f552247b5\", \"compound_pp_quality\": 0.9707314591018954, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.8799559166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0f05c645-aec1-44e0-8c53-354e85a38632\", \"compound_pp_quality\": 0.9718387014963226, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5622455, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a94c59a1-d2c5-4486-9e86-ad14dcbd3d5f\", \"compound_pp_quality\": 0.9710421962379259, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.3591971833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"745ef5aa-d730-40d9-b6f6-1d463b8154eb\", \"compound_pp_quality\": 0.9696583375828659, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5803049, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ea9b7326-76d0-4013-a553-e57c2d4a3a0a\", \"compound_pp_quality\": 0.9693877551020409, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.9064114166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"4e41033f-a5d5-43f5-b448-edf568bc69a7\", \"compound_pp_quality\": 0.970805241063478, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.82476975, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"d55423d6-8f42-465f-ae93-e5c460fce626\", \"compound_pp_quality\": 0.9724828810550342, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.1378944166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e\", \"compound_pp_quality\": 0.9712760549059481, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4552481833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"Selection_Rate_Difference\",\n",
    "                                           group=\"male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbc888e487d531e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:51.140742Z",
     "start_time": "2024-12-23T20:20:51.087453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-72e75778d6754dc0af3cd89405233a93\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-72e75778d6754dc0af3cd89405233a93\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-72e75778d6754dc0af3cd89405233a93\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 22, \"labelFontWeight\": \"normal\", \"titleFontSize\": 28, \"titleFontWeight\": \"normal\"}, \"title\": {\"fontSize\": 28}}, \"data\": {\"name\": \"data-74932b799c91a26f5f1cca616279431d\"}, \"mark\": {\"type\": \"boxplot\", \"color\": \"orange\", \"median\": {\"stroke\": \"black\", \"strokeWidth\": 0.7}, \"ticks\": true}, \"encoding\": {\"x\": {\"axis\": {\"labelAngle\": -45, \"labelLimit\": 500}, \"field\": \"exp_config_name\", \"title\": null, \"type\": \"nominal\"}, \"y\": {\"field\": \"disparity_metric_value\", \"scale\": {\"zero\": false}, \"title\": \"SRD\", \"type\": \"quantitative\"}}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-74932b799c91a26f5f1cca616279431d\": [{\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a9366a20-5bb4-4743-b105-7963ecf20c46\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.6189442666666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"2326be7a-8b8a-4dc8-971f-d5909fe4476b\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4694882166666665, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6c206ee9-1583-437f-b436-5296f847f025\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.9273404833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f9a6b87a-bdf5-4a8c-8f65-0f31d56c6bc6\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.66953015, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_25_fair2_0_5\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"f53a74fc-cb9d-4153-b9af-611df3acd7bf\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.649014166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"9ecb20b1-3d80-4c3a-9051-80bc4d6da81c\", \"compound_pp_quality\": 0.984829168791433, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5480944, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"765e8ea9-7b45-4b00-8e13-ac302a999fc8\", \"compound_pp_quality\": 0.9846938775510204, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.9093007, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"c0bc36ff-a974-4193-a770-431197c698be\", \"compound_pp_quality\": 0.985402620531739, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.7651768666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0b5be382-b07c-4265-b6e5-97aaa2fb5f7f\", \"compound_pp_quality\": 0.986241440527517, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.0431344, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_25_fair1_0_5_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7eec8589-e4f6-42db-9649-3448f6e0353f\", \"compound_pp_quality\": 0.9856380274529741, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.3439765166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"75ef957d-32a2-4110-8b5c-354831393ebe\", \"compound_pp_quality\": 0.9699745028046916, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4616962666666666, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"7aa403fe-2a72-487d-9cb5-58ae6c5a4750\", \"compound_pp_quality\": 0.969795918367347, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.7881325833333332, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"5a02b83a-7822-40fb-a34a-2f3f552247b5\", \"compound_pp_quality\": 0.9707314591018954, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.8799559166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"0f05c645-aec1-44e0-8c53-354e85a38632\", \"compound_pp_quality\": 0.9718387014963226, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5622455, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_33_fair1_0_33_fair2_0_33\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"a94c59a1-d2c5-4486-9e86-ad14dcbd3d5f\", \"compound_pp_quality\": 0.9710421962379259, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.3591971833333334, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 1, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"745ef5aa-d730-40d9-b6f6-1d463b8154eb\", \"compound_pp_quality\": 0.9696583375828659, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.5803049, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 2, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"ea9b7326-76d0-4013-a553-e57c2d4a3a0a\", \"compound_pp_quality\": 0.9693877551020409, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.9064114166666668, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 3, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"4e41033f-a5d5-43f5-b448-edf568bc69a7\", \"compound_pp_quality\": 0.970805241063478, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 0.82476975, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 4, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"d55423d6-8f42-465f-ae93-e5c460fce626\", \"compound_pp_quality\": 0.9724828810550342, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 2.1378944166666667, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}, {\"exp_config_name\": \"acc_0_5_fair1_0_25_fair2_0_25\", \"run_num\": 5, \"logical_pipeline_uuid\": \"Tm9uZSZOT19GQUlSTkVTU19JTlRFUlZFTlRJT04mZ2FuZGFsZl9jbGY=\", \"physical_pipeline_uuid\": \"6b3e8dd5-2ae8-4e76-84dd-7e69bdd1708e\", \"compound_pp_quality\": 0.9712760549059481, \"metric\": \"Selection_Rate_Difference\", \"model_name\": \"gandalf_clf\", \"runtime_in_mins\": 1.4552481833333333, \"dataset_name\": \"law_school\", \"logical_pipeline_name\": \"None&NO_FAIRNESS_INTERVENTION&gandalf_clf\", \"null_imputer_name\": \"None\", \"fairness_intervention_name\": \"NO_FAIRNESS_INTERVENTION\", \"male&race_dis\": 1.0, \"male&race_priv\": 1.0, \"male_dis\": 1.0, \"male_priv\": 1.0, \"overall\": 1.0, \"race_dis\": 1.0, \"race_priv\": 1.0, \"disparity_metric_value\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_box_plot_per_dataset_and_case_study(to_plot=best_lp_metrics_per_exp_config_df,\n",
    "                                           exp_name=EXP_NAME,\n",
    "                                           dataset_name=DATASET_NAME,\n",
    "                                           case_study_name=CASE_STUDY_NAME,\n",
    "                                           metric_name=\"Selection_Rate_Difference\",\n",
    "                                           group=\"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ee318769d68b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:08.300525Z",
     "start_time": "2024-12-23T10:43:41.304994Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
